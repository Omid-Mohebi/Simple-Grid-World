{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "actions = ['up', 'down', 'left', 'right']\n",
    "row = 3\n",
    "column = 4\n",
    "start_state = (2, 0)\n",
    "win_state = (0, 3)\n",
    "lose_state = (1, 3)\n",
    "wall = (1, 1)\n",
    "policy = np.random.choice(actions, (row, column))\n",
    "v_s = np.zeros((row, column))\n",
    "tetha = 1e-5\n",
    "gamma = 0.9\n",
    "epsilon = 0.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def move(a, r, c):\n",
    "    current_state = (r, c)\n",
    "    if current_state == wall:\n",
    "        return current_state\n",
    "    new_r, new_c = r, c\n",
    "    if a == 'up':\n",
    "        new_r = max(r-1, 0)\n",
    "    elif a == 'down':\n",
    "        new_r = min(r+1, row-1)\n",
    "    elif a == 'right':\n",
    "        new_c = min(c+1, column-1)\n",
    "    elif a == 'left':\n",
    "        new_c = max(c-1, 0)\n",
    "    new_state = (new_r, new_c)\n",
    "    if new_state == wall:\n",
    "        return current_state\n",
    "    else:\n",
    "        return new_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def end_game(state):\n",
    "    return state == win_state or state == lose_state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reward(state):\n",
    "    if state == win_state:\n",
    "        return 1\n",
    "    elif state == lose_state:\n",
    "        return -1\n",
    "    return -0.04"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_eva():\n",
    "    global v_s\n",
    "    returns_sum = np.zeros((row, column))\n",
    "    returns_count = np.zeros((row, column))\n",
    "    \n",
    "    for _ in range(5000):\n",
    "        state = start_state\n",
    "        episode_states = []\n",
    "        episode_rewards = []\n",
    "        steps = 0\n",
    "        \n",
    "        while not end_game(state) and steps < 100:\n",
    "            action = policy[state[0], state[1]]\n",
    "            next_state = move(action, state[0], state[1])\n",
    "            r = reward(next_state)\n",
    "\n",
    "            episode_states.append(state)\n",
    "            episode_rewards.append(r)\n",
    "            state = next_state\n",
    "            steps += 1\n",
    "        \n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode_states))):\n",
    "            s = episode_states[t]\n",
    "            G = episode_rewards[t] + gamma * G\n",
    "            if s not in visited:\n",
    "                visited.add(s)\n",
    "                returns_sum[s[0], s[1]] += G\n",
    "                returns_count[s[0], s[1]] += 1\n",
    "    \n",
    "    for r in range(row):\n",
    "        for c in range(column):\n",
    "            if returns_count[r, c] > 0:\n",
    "                v_s[r, c] = returns_sum[r, c] / returns_count[r, c]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "monte_carlo_policy_eva()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Value function:\n",
      "[[ 0.          0.          0.          0.        ]\n",
      " [-0.04        0.          0.          0.        ]\n",
      " [-0.39998938  0.          0.          0.        ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Value function:\")\n",
    "print(v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([['down', 'up', 'up', 'right'],\n",
       "       ['left', 'left', 'up', 'left'],\n",
       "       ['up', 'down', 'down', 'left']], dtype='<U5')"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_init_state():\n",
    "    while True:\n",
    "        state = (np.random.randint(0, row), np.random.randint(0, column))\n",
    "        if (state != wall) and (state != win_state) and (state != lose_state):\n",
    "            return state\n",
    "\n",
    "def random_action():\n",
    "    return np.random.choice(actions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def monte_carlo_policy_imp():\n",
    "    global policy\n",
    "    Q = np.zeros((row, column, len(actions)))\n",
    "    returns = {((r, c), a): [] for r in range(row) for c in range(column) for a in actions}\n",
    "\n",
    "    for _ in range(3000):\n",
    "        state = random_init_state()\n",
    "        action = random_action()\n",
    "        episode = []\n",
    "        steps = 0\n",
    "\n",
    "        while not end_game(state) and steps < 100:\n",
    "            next_state = move(action, state[0], state[1])\n",
    "            r = reward(next_state)\n",
    "            episode.append((state, action, r))\n",
    "            state = next_state\n",
    "            action = random_action()\n",
    "            steps += 1\n",
    "\n",
    "        G = 0\n",
    "        visited = set()\n",
    "        for t in reversed(range(len(episode))):\n",
    "            state, action, r = episode[t]\n",
    "            G = r + gamma * G\n",
    "            state_action = (state, action)\n",
    "            if state_action not in visited:\n",
    "                visited.add(state_action)\n",
    "                returns[state, action].append(G)\n",
    "                Q[state[0], state[1], actions.index(action)] = np.mean(returns[state, action])\n",
    "\n",
    "        for r in range(row):\n",
    "            for c in range(column):\n",
    "                if (r, c) == wall or end_game((r, c)):\n",
    "                    continue\n",
    "                best_action_index = np.argmax(Q[r, c])\n",
    "                \n",
    "                if np.random.rand() < epsilon:\n",
    "                    policy[r, c] = np.random.choice(actions)\n",
    "                else:\n",
    "                    policy[r, c] = actions[best_action_index]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(10):\n",
    "    monte_carlo_policy_imp()\n",
    "    if i % 2 == 0:\n",
    "        monte_carlo_policy_eva()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Final Policy:\n",
      "[['right' 'right' 'right' 'right']\n",
      " ['up' 'left' 'up' 'left']\n",
      " ['up' 'up' 'left' 'left']]\n",
      "\n",
      "Value function:\n",
      "[[0.734   0.86    1.      0.     ]\n",
      " [0.6206  0.      0.      0.     ]\n",
      " [0.51854 0.      0.      0.     ]]\n"
     ]
    }
   ],
   "source": [
    "print(\"Final Policy:\")\n",
    "print(policy)\n",
    "print(\"\\nValue function:\")\n",
    "print(v_s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
